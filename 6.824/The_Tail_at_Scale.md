
## Why Variability Exists

* 全局共享资源
* 后台活动（数据重排，周期性日志压缩，周期性垃圾回收）
* 请求排队
* 功率限制（现代CPU的频率是周期性的）
* 垃圾回收（所有软删除状态的垃圾回收系统）
* 能耗管理（能耗管理程序会使CPU降频）

## Component-Level Variability Amplified By Scale

在分布式系统中，请求分发到每个服务器上，假如一个请求的99线在1秒钟，一个需要进行100个并行请求的任务，这个任务的延迟有63%的可能性超过1秒。

## Reducing Component Variability

* 带优先级的服务请求队列。优先级更高的服务能够得到更快的响应。
* 降低队头阻塞。考虑到某些长耗时的请求阻塞掉后续的短耗时的请求，根据耗时不一样拆分成不同的请求队列，来提高大部分短耗时请求的延迟。
* 管理后台活动。对于周期性的日志回收或者垃圾收集这种活动，服务可以选择在服务器Load低水平时主动触发，把大的活动拆分成小的活动。对于扇出型的服务，要求所有服务器的后台服务在同时运行，避免在某些时刻总有一些服务在做背景活动导致扇出请求的长尾效应。

## Living with Latency Variability

延迟的可变性无法彻底的消除，我们只能寻求长期共存的方式，容忍延迟的可变性。

## Within Request Short-Term Adaptations

### Hedged requests

发送请求后，在指定时间内没有返回，重新发送一个请求。这种技术的前提是大部分情形下，请求延迟很高并不是这个请求本身处理时间很长，而是额外原因引起的。

更加实用的技术是选择95线的超时请求，这样能有效的降低长尾时间，而且只增加了5%的额外请求。

### Tied requests

有相当的延迟来源于请求排队，所以，有一种绑定请求的方式。可以一次发送多个请求，服务端能够进行交互，当某个服务器开始执行请求时，发送取消请求到其它收到该请求的服务器。这种手段能有效降低尾延迟，而且不会增加更多的额外负担。客户端可以选择一个较小的延迟来发送多个请求，比如1ms，这个延迟取决于机房网的消息延迟。

选择低负载机器来发送请求，在实际使用效果上不比上面两个手段更好。原因有：负载可能随时会变，而且这个变化很随机。无法区分负载是底层硬件还是处理请求数量的问题。客户端的刻意选择会造成低负载机器可能在接下来又变成热点机。

## Cross-Request Long-Term Adaptations

### Micro-partitions

微分区，数据首先分配成数量级别远超服务器的片，然后片动态的分配到服务器上。
在动态平衡过程中，数据集的大小会发生变化，按照这种分片方式，在数据发生平衡迁移时，能够降低迁移数据的工作量。

## Selective replication

选择分片，对于一些热点数据集，如果采用动态负载的话，可能会聚集到一个服务器上。引入可强制选择的机制，能够按照需求来调整热点数据集的分配。

## Latency-induced probation

基于性能监控，可以通过统计所有服务器的延迟数据，来选择分配服务器的请求数量。

## Large Information Retrieval Systems

在大型信息检索系统中，一个查询会分解成诸多请求到不同的服务器上运行，然后收集所有结果得到响应。

Good enough。与其等待所有的服务器，包括那些很慢的服务器的返回结果进行聚合，不如挑选返回很快的服务器中部分结果进行聚合直接返回。

Canary requests。 金丝雀请求，在有些查询发出的大型广播请求中，有些代码和规则没有经过覆盖测试，这种查询可能会导致所有的请求都挂掉。所以，根服务器需要试探性的发出两个请求到两三台服务器上，如果在指定的时间没有完成请求或者出现崩溃异常，根服务器会标记该查询是非法的，然后关闭请求。这种手段会提高延迟，但是能够降低大量的无效查询，保护服务器资源。

